{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arbitrary-horror",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/E2E/dataset/testing/calib/000000.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f5af980022cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkitti_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcalib_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/E2E/dataset/testing/calib/000000.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcalib_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkitti_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalib_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/E2E/utils/kitti_util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, calib_filepath, from_video)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mcalibs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_calib_from_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalib_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mcalibs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_calib_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalib_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Projection matrix from rect camera coord to image2 coord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'P2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/E2E/utils/kitti_util.py\u001b[0m in \u001b[0;36mread_calib_file\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    135\u001b[0m         '''\n\u001b[1;32m    136\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/E2E/dataset/testing/calib/000000.txt'"
     ]
    }
   ],
   "source": [
    "sys.path.append('/notebooks/E2E')\n",
    "sys.path.append('/notebooks/')\n",
    "import utils.kitti_util as util\n",
    "\n",
    "import kitti_util \n",
    "\n",
    "calib_info = util.Calibration(\"/notebooks/E2E/dataset/testing/calib/000000.txt\")\n",
    "\n",
    "calib_info = kitti_util.Calib(calib_info)\n",
    "image = torch.from_numpy(read_img(\"/notebooks/E2E/dataset/testing/image_2/000000.png\"))\n",
    "W, H = image.size\n",
    "depth = depth[-H:, :W]\n",
    "cloud = depth_to_pcl(calib_info, depth, max_high=max_high)\n",
    "cloud = filter_cloud(cloud, image, calib_info)\n",
    "cloud = transform(cloud, calib_info, sparse_type='angular_min', start=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "allied-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "def read_img(filename):\n",
    "    # Convert to RGB for scene flow finalpass data\n",
    "    img = np.array(Image.open(filename).convert('RGB')).astype(np.float32)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "derived-orlando",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'depth_network'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-16ae2eb97063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdepth_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'depth_network'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from depth_network import logger\n",
    "import os\n",
    "import shutil\n",
    "from depth_network.models import *\n",
    "import kitti_util\n",
    "import batch_utils\n",
    "\n",
    "from PIL import Image\n",
    "from tensorboardX import SummaryWriter\n",
    "import ipdb\n",
    "\n",
    "\n",
    "def loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "\n",
    "def dynamic_baseline(calib):\n",
    "    P3 = calib.P3\n",
    "    P = calib.P2\n",
    "    baseline = P3[0, 3] / (-P3[0, 0]) - P[0, 3] / (-P[0, 0])\n",
    "    return baseline\n",
    "\n",
    "\n",
    "class DepthModel():\n",
    "    def __init__(self, maxdisp, down, maxdepth, pretrain, save_tag, mode='TRAIN', dynamic_bs=False,\n",
    "                     lr=0.001, mgpus=False, lr_stepsize=[10, 20], lr_gamma=0.1):\n",
    "\n",
    "        result_dir = os.path.join('../', 'output', 'depth', save_tag)\n",
    "        # set logger\n",
    "        log = logger.setup_logger(os.path.join(result_dir, 'training.log'))\n",
    "\n",
    "        # set tensorboard\n",
    "        writer = SummaryWriter(result_dir + '/tensorboardx')\n",
    "\n",
    "        model = stackhourglass(maxdisp, down=down, maxdepth=maxdepth)\n",
    "\n",
    "        # Number of parameters\n",
    "        log.info('Number of model parameters: {}'.format(\n",
    "            sum([p.data.nelement() for p in model.parameters()])))\n",
    "        if mgpus or mode == 'TEST':\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "        scheduler = MultiStepLR(\n",
    "            optimizer, milestones=lr_stepsize, gamma=lr_gamma)\n",
    "\n",
    "        if pretrain is not None:\n",
    "            if os.path.isfile(pretrain):\n",
    "                log.info(\"=> loading pretrain '{}'\".format(pretrain))\n",
    "                checkpoint = torch.load(pretrain)\n",
    "                if mgpus or mode == 'TEST':\n",
    "                    model.load_state_dict(checkpoint['state_dict'])\n",
    "                else:\n",
    "                    model.load_state_dict(self.strip_prefix(checkpoint['state_dict']))\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "            else:\n",
    "                log.info(\n",
    "                    '[Attention]: Do not find checkpoint {}'.format(pretrain))\n",
    "\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.net = model\n",
    "        self.dynamic_bs = dynamic_bs\n",
    "        self.mode = mode\n",
    "        self.result_dir = result_dir\n",
    "\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    def load_data(self, batch_left_img, batch_right_img, batch_gt_depth, batch_calib):\n",
    "        left_imgs, right_imgs, calibs = [], [], []\n",
    "        for left_img, right_img, calib in zip(\n",
    "                batch_left_img, batch_right_img, batch_calib):\n",
    "            if self.dynamic_bs:\n",
    "                calib = calib.P2[0, 0] * dynamic_baseline(calib)\n",
    "            else:\n",
    "                calib = calib.P2[0, 0] * 0.54\n",
    "\n",
    "            calib = torch.tensor(calib)\n",
    "            left_img = self.img_transform(left_img)\n",
    "            right_img = self.img_transform(right_img)\n",
    "\n",
    "            # pad to (384, 1248)\n",
    "            C, H, W = left_img.shape\n",
    "            top_pad = 384 - H\n",
    "            right_pad = 1248 - W\n",
    "            left_img = F.pad(\n",
    "                left_img, (0, right_pad, top_pad, 0), \"constant\", 0)\n",
    "            right_img = F.pad(\n",
    "                right_img, (0, right_pad, top_pad, 0), \"constant\", 0)\n",
    "\n",
    "            left_imgs.append(left_img)\n",
    "            right_imgs.append(right_img)\n",
    "            calibs.append(calib)\n",
    "\n",
    "        left_img = torch.stack(left_imgs)\n",
    "        right_img = torch.stack(right_imgs)\n",
    "        calib = torch.stack(calibs)\n",
    "\n",
    "        gt_depth = torch.from_numpy(batch_gt_depth).cuda(non_blocking=True)\n",
    "\n",
    "        return left_img.float(), right_img.float(), gt_depth.float(), calib.float()\n",
    "\n",
    "    \n",
    "    def strip_prefix(self, state_dict, prefix='module.'):\n",
    "        if not all(key.startswith(prefix) for key in state_dict.keys()):\n",
    "            return state_dict\n",
    "        stripped_state_dict = {}\n",
    "        for key in list(state_dict.keys()):\n",
    "            stripped_state_dict[key.replace(prefix, '')] = state_dict.pop(key)\n",
    "        return stripped_state_dict\n",
    "\n",
    "\n",
    "def depth_to_pcl(calib, depth, max_high=1.):\n",
    "    rows, cols = depth.shape\n",
    "    c, r = torch.meshgrid(torch.arange(0., cols, device='cuda'),\n",
    "                          torch.arange(0., rows, device='cuda'))\n",
    "    points = torch.stack([c.t(), r.t(), depth], dim=0)\n",
    "    points = points.reshape((3, -1))\n",
    "    points = points.t()\n",
    "    cloud = calib.img_to_lidar(points[:, 0], points[:, 1], points[:, 2])\n",
    "    valid = (cloud[:, 0] >= 0) & (cloud[:, 2] < max_high)\n",
    "    lidar = cloud[valid]\n",
    "\n",
    "    # pad 1 in the intensity dimension\n",
    "    lidar = torch.cat(\n",
    "        [lidar, torch.ones((lidar.shape[0], 1), device='cuda')], 1)\n",
    "    lidar = lidar.float()\n",
    "    return lidar\n",
    "\n",
    "\n",
    "def transform(points, calib_info, sparse_type, start=2.):\n",
    "    if sparse_type == 'angular':\n",
    "        points = random_sparse_angular(points)\n",
    "    if sparse_type == 'angular_min':\n",
    "        points = nearest_sparse_angular(points, start)\n",
    "    if sparse_type == 'angular_numpy':\n",
    "        points = points.cpu().numpy()\n",
    "        points = pto_ang_map(points).astype(np.float32)\n",
    "        points = torch.from_numpy(points).cuda()\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def filter_cloud(velo_points, image, calib):\n",
    "    W, H = image.size\n",
    "    _, _, valid_inds_fov = get_lidar_in_image_fov(\n",
    "        velo_points[:, :3], calib, 0, 0, W, H, True)\n",
    "    velo_points = velo_points[valid_inds_fov]\n",
    "\n",
    "    # depth, width, height\n",
    "    valid_inds = (velo_points[:, 0] < 120) & \\\n",
    "                 (velo_points[:, 0] >= 0) & \\\n",
    "                 (velo_points[:, 1] < 50) & \\\n",
    "                 (velo_points[:, 1] >= -50) & \\\n",
    "                 (velo_points[:, 2] < 1.5) & \\\n",
    "                 (velo_points[:, 2] >= -2.5)\n",
    "    velo_points = velo_points[valid_inds]\n",
    "    return velo_points\n",
    "\n",
    "\n",
    "def gen_ang_map(velo_points, start=2., H=64, W=512, device='cuda'):\n",
    "    dtheta = math.radians(0.4 * 64.0 / H)\n",
    "    dphi = math.radians(90.0 / W)\n",
    "\n",
    "    x, y, z, i = velo_points[:, 0], velo_points[:,\n",
    "                                    1], velo_points[:, 2], velo_points[:, 3]\n",
    "\n",
    "    d = torch.sqrt(x ** 2 + y ** 2 + z ** 2)\n",
    "    r = torch.sqrt(x ** 2 + y ** 2)\n",
    "    d[d == 0] = 0.000001\n",
    "    r[r == 0] = 0.000001\n",
    "    phi = math.radians(45.) - torch.asin(y / r)\n",
    "    phi_ = (phi / dphi).long()\n",
    "    phi_ = torch.clamp(phi_, 0, W - 1)\n",
    "\n",
    "    theta = math.radians(start) - torch.asin(z / d)\n",
    "    theta_ = (theta / dtheta).long()\n",
    "    theta_ = torch.clamp(theta_, 0, H - 1)\n",
    "    return [theta_, phi_]\n",
    "\n",
    "\n",
    "def random_sparse_angular(velo_points, H=64, W=512, slice=1, device='cuda'):\n",
    "    \"\"\"\n",
    "    :param velo_points: Pointcloud of size [N, 4]\n",
    "    :param H: the row num of depth map, could be 64(default), 32, 16\n",
    "    :param W: the col num of depth map\n",
    "    :param slice: output every slice lines\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        theta_, phi_ = gen_ang_map(velo_points, H=64, W=512, device=device)\n",
    "\n",
    "    depth_map = - torch.ones((H, W, 4), device=device)\n",
    "\n",
    "    depth_map = depth_map\n",
    "    velo_points = velo_points\n",
    "    x, y, z, i = velo_points[:, 0], velo_points[:, 1], velo_points[:, 2], velo_points[:, 3]\n",
    "    theta_, phi_ = theta_, phi_\n",
    "\n",
    "    # Currently, does random subsample (maybe keep the points with min distance)\n",
    "    depth_map[theta_, phi_, 0] = x\n",
    "    depth_map[theta_, phi_, 1] = y\n",
    "    depth_map[theta_, phi_, 2] = z\n",
    "    depth_map[theta_, phi_, 3] = i\n",
    "    depth_map = depth_map.cuda()\n",
    "\n",
    "    depth_map = depth_map[0:: slice, :, :]\n",
    "    depth_map = depth_map.reshape((-1, 4))\n",
    "    return depth_map[depth_map[:, 0] != -1.0]\n",
    "\n",
    "\n",
    "\n",
    "def pto_ang_map(velo_points, H=64, W=512, slice=1):\n",
    "    \"\"\"\n",
    "    :param H: the row num of depth map, could be 64(default), 32, 16\n",
    "    :param W: the col num of depth map\n",
    "    :param slice: output every slice lines\n",
    "    \"\"\"\n",
    "\n",
    "#   np.random.shuffle(velo_points)\n",
    "    dtheta = np.radians(0.4 * 3.0 / H)\n",
    "    dphi = np.radians(90.0 / W)\n",
    "\n",
    "    x, y, z, i = velo_points[:, 0], velo_points[:, 1], velo_points[:, 2], velo_points[:, 3]\n",
    "\n",
    "    d = np.sqrt(x ** 2 + y ** 2 + z ** 2)\n",
    "    r = np.sqrt(x ** 2 + y ** 2)\n",
    "    d[d == 0] = 0.000001\n",
    "    r[r == 0] = 0.000001\n",
    "    phi = np.radians(45.) - np.arcsin(y / r)\n",
    "    phi_ = (phi / dphi).astype(int)\n",
    "    phi_[phi_ < 0] = 0\n",
    "    phi_[phi_ >= W] = W - 1\n",
    "\n",
    "    theta = np.radians(2.) - np.arcsin(z / d)\n",
    "    theta_ = (theta / dtheta).astype(int)\n",
    "    theta_[theta_ < 0] = 0\n",
    "    theta_[theta_ >= H] = H - 1\n",
    "\n",
    "    depth_map = - np.ones((H, W, 4))\n",
    "    depth_map[theta_, phi_] = velo_points\n",
    "\n",
    "    depth_map = depth_map[0::slice, :, :]\n",
    "    depth_map = depth_map.reshape((-1, 4))\n",
    "    depth_map = depth_map[depth_map[:, 0] != -1.0]\n",
    "    return depth_map\n",
    "\n",
    "\n",
    "def nearest_sparse_angular(velo_points, start=2., H=64, W=512, slice=1, device='cuda'):\n",
    "    \"\"\"\n",
    "    :param H: the row num of depth map, could be 64(default), 32, 16\n",
    "    :param W: the col num of depth map\n",
    "    :param slice: output every slice lines\n",
    "    \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        theta_, phi_ = gen_ang_map(velo_points, start, H, W, device=device)\n",
    "\n",
    "    depth_map = - torch.ones((H, W, 4), device=device)\n",
    "    depth_map = min_dist_subsample(velo_points, theta_, phi_, H, W, device='cuda')\n",
    "    # depth_map = depth_map[0::slice, :, :]\n",
    "    depth_map = depth_map.reshape((-1, 4))\n",
    "    sparse_points = depth_map[depth_map[:, 0] != -1.0]\n",
    "    return sparse_points\n",
    "\n",
    "\n",
    "def min_dist_subsample(velo_points, theta_, phi_, H, W, device='cuda'):\n",
    "    N = velo_points.shape[0]\n",
    "\n",
    "    idx = theta_ * W + phi_  # phi_ in range [0, W-1]\n",
    "    depth = torch.arange(0, N, device='cuda')\n",
    "\n",
    "    sampled_depth, argmin = scatter_max(depth, idx)\n",
    "    mask = argmin[argmin != -1]\n",
    "    return velo_points[mask]\n",
    "\n",
    "\n",
    "def save_pcl(point_cloud, path='point'):\n",
    "    point_cloud = point_cloud.detach().cpu()\n",
    "    np.save(path, point_cloud)\n",
    "\n",
    "\n",
    "def get_lidar_in_image_fov(pc_velo, calib, xmin, ymin, xmax, ymax,\n",
    "                           return_more=False, clip_distance=2.0):\n",
    "    ''' Filter lidar points, keep those in image FOV '''\n",
    "    pts_2d, pts_rect_depth = calib.lidar_to_img(pc_velo)\n",
    "    fov_inds = (pts_2d[:, 0] < xmax) & (pts_2d[:, 0] >= xmin) & \\\n",
    "        (pts_2d[:, 1] < ymax) & (pts_2d[:, 1] >= ymin)\n",
    "    fov_inds = fov_inds & (pc_velo[:, 0] > clip_distance)\n",
    "    imgfov_pc_velo = pc_velo[fov_inds, :]\n",
    "    if return_more:\n",
    "        return imgfov_pc_velo, pts_2d, fov_inds\n",
    "    else:\n",
    "        return imgfov_pc_velo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-norman",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btngan1",
   "language": "python",
   "name": "btngan1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
